{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6e746ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "# エンコーディング自動検出用\n",
    "from charset_normalizer import from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b0990823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLファイルが格納されているディレクトリのパス\n",
    "directory = \"www.geocities.co.jp/Playtown-Dice/6159\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fd025171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mojibake(text):\n",
    "    \"\"\"\n",
    "    簡易的な文字化け検出関数。非表示文字や無効な文字がある場合にTrueを返す。\n",
    "    \"\"\"\n",
    "    # 文字化けによく見られる文字パターンを除外（ここでは基本的な制御文字を例に）\n",
    "    mojibake_patterns = [\n",
    "        r'[�]'  # 置換文字\n",
    "    ]\n",
    "    \n",
    "    for pattern in mojibake_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "00c81c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_with_fallback(file_path: str) -> BeautifulSoup:\n",
    "    # 優先的に Shift_JIS / CP932 を試す\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"shift_jis\") as f:\n",
    "            content = f.read()\n",
    "        return BeautifulSoup(content, \"html.parser\")\n",
    "    except UnicodeDecodeError:\n",
    "        pass  # Shift_JIS 読み込みに失敗した場合は次へ\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"cp932\") as f:\n",
    "            content = f.read()\n",
    "        return BeautifulSoup(content, \"html.parser\")\n",
    "    except UnicodeDecodeError:\n",
    "        pass  # CP932 でも失敗した場合は自動判定へ\n",
    "\n",
    "    # charset_normalizer による自動検出\n",
    "    result = from_path(file_path).best()\n",
    "    if result is None:\n",
    "        raise ValueError(f\"エンコーディングの検出に失敗しました: {file_path}\")\n",
    "\n",
    "    content = result.decoded\n",
    "    return BeautifulSoup(content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f5b2fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_entries_from_html(file_path):\n",
    "    \"\"\"\n",
    "    HTML内のすべての trペア から {title, issue_info, body_text} を抽出する\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = read_html_with_fallback(file_path)\n",
    "\n",
    "        rows = soup.find_all(\"tr\")\n",
    "        entries = []\n",
    "\n",
    "        # ペアで処理（title/infoが1行目, 本文が2行目）\n",
    "        for i in range(0, len(rows) - 1, 2):\n",
    "            tr_title = rows[i]\n",
    "            tr_body = rows[i + 1]\n",
    "\n",
    "            # タイトル\n",
    "            title_tag = tr_title.find(\"font\", attrs={\"size\": \"+2\"})\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # 出典\n",
    "            issue_tag = tr_title.find(\"font\", attrs={\"size\": \"-1\"})\n",
    "            issue_info = issue_tag.get_text(strip=True) if issue_tag else None\n",
    "\n",
    "            # 本文（1つまたは複数）\n",
    "            # span.line タグが壊れてる可能性に備え、tr_body 全体を文字列として処理\n",
    "            raw_html = str(tr_body)\n",
    "\n",
    "            # <span class=\"line\">〜(次の <span> or </tr> or <td> まで)\n",
    "            raw_spans = re.findall(r'<span class=\"line\">(.*?)(?=<span class=\"line\">|<td bgcolor=\"#80ffff\">|</tr>|</td>)', raw_html, re.DOTALL)\n",
    "            # HTMLタグを除去して本文だけにする\n",
    "            body_texts = [BeautifulSoup(fragment, \"html.parser\").get_text(separator=\" \", strip=True) for fragment in raw_spans]\n",
    "\n",
    "            # 最初の要素がお目当ての本文なので取得\n",
    "            body_text = body_texts[0]\n",
    "\n",
    "            entries.append({\n",
    "                \"title\": title,\n",
    "                \"issue_info\": issue_info,\n",
    "                \"body_text\": body_text\n",
    "            })\n",
    "\n",
    "        return entries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0ffbf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_filename(file: str) -> int | None:\n",
    "    match = re.match(r\"^d-(\\d+)\\.html?$\", file)\n",
    "    return int(match.group(1)) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2f3ba9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_xx_recursive_to_df(directory, max_pages=1000):\n",
    "    \"\"\"\n",
    "    指定されたディレクトリを再帰的に検索し、HTMLファイルからテキストを抽出し、\n",
    "    ディレクトリ構造をキーとしてDataFrameに格納する関数\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    page_count = 0\n",
    "    \n",
    "    for root, _dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            match_index = extract_number_from_filename(file)\n",
    "            if match_index:\n",
    "                file_path = os.path.join(root, file)\n",
    "                stories = extract_all_entries_from_html(file_path)\n",
    "                if len(stories) == 0: continue\n",
    "                # ディレクトリ構造と抽出したテキストをDataFrameに格納するためリストに追加\n",
    "                for index, story in enumerate(stories, 1):\n",
    "                    data.append({\n",
    "                        \"directory\": root,\n",
    "                        # \"dirs\": _dirs,\n",
    "                        \"file\": file,\n",
    "                        \"volume\": match_index,\n",
    "                        \"story_index\": index,\n",
    "                        \"title\": story[\"title\"],\n",
    "                        \"issue_info\": story[\"issue_info\"],\n",
    "                        \"body_text\": story[\"body_text\"]\n",
    "                    })\n",
    "                    \n",
    "                page_count += 1\n",
    "                if page_count >= max_pages:\n",
    "                    print(f\"Processed {max_pages} pages. Stopping...\")\n",
    "                    return pd.DataFrame(data)  # DataFrameに変換して返す\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1f5b5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行\n",
    "df = extract_d_xx_recursive_to_df(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c3ca0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"volume\", \"story_index\"], inplace=True, ignore_index=True)\n",
    "# データフレームを表示\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1c3516d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"doraemon_stories.csv\", index=False, escapechar='\\\\', quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
