{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6e746ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "# エンコーディング自動検出用\n",
    "from charset_normalizer import from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0990823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLファイルが格納されているディレクトリのパス\n",
    "directory = \"www.geocities.co.jp/Playtown-Dice/6159\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fd025171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mojibake(text):\n",
    "    \"\"\"\n",
    "    簡易的な文字化け検出関数。非表示文字や無効な文字がある場合にTrueを返す。\n",
    "    \"\"\"\n",
    "    # 文字化けによく見られる文字パターンを除外（ここでは基本的な制御文字を例に）\n",
    "    mojibake_patterns = [\n",
    "        r'[�]'  # 置換文字\n",
    "    ]\n",
    "    \n",
    "    for pattern in mojibake_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f5b2fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_entries_from_html(file_path):\n",
    "    \"\"\"\n",
    "    HTML内のすべての trペア から {title, issue_info, body_text} を抽出する\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = from_path(file_path).best()\n",
    "        content = str(result)\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        rows = soup.find_all(\"tr\")\n",
    "        entries = []\n",
    "\n",
    "        # ペアで処理（title/infoが1行目, 本文が2行目）\n",
    "        for i in range(0, len(rows) - 1, 2):\n",
    "            tr_title = rows[i]\n",
    "            tr_body = rows[i + 1]\n",
    "\n",
    "            # タイトル\n",
    "            title_tag = tr_title.find(\"font\", attrs={\"size\": \"+2\"})\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # 出典\n",
    "            issue_tag = tr_title.find(\"font\", attrs={\"size\": \"-1\"})\n",
    "            issue_info = issue_tag.get_text(strip=True) if issue_tag else None\n",
    "\n",
    "            # 本文（1つまたは複数）\n",
    "            # span.line タグが壊れてる可能性に備え、tr_body 全体を文字列として処理\n",
    "            raw_html = str(tr_body)\n",
    "\n",
    "            # <span class=\"line\">〜(次の <span> or </tr> or <td> まで)\n",
    "            raw_spans = re.findall(r'<span class=\"line\">(.*?)(?=<span class=\"line\">|<td bgcolor=\"#80ffff\">|</tr>|</td>)', raw_html, re.DOTALL)\n",
    "            # HTMLタグを除去して本文だけにする\n",
    "            body_texts = [BeautifulSoup(fragment, \"html.parser\").get_text(separator=\" \", strip=True) for fragment in raw_spans]\n",
    "\n",
    "            body_text = body_texts[0]\n",
    "\n",
    "            full_text = f\"{title or ''}\\n{issue_info or ''}\\n{body_text or ''}\"\n",
    "            if is_mojibake(full_text):\n",
    "                print(f\"Skipping entry due to mojibake at index {i}\")\n",
    "                continue\n",
    "\n",
    "            entries.append({\n",
    "                \"title\": title,\n",
    "                \"issue_info\": issue_info,\n",
    "                \"body_text\": body_text\n",
    "            })\n",
    "\n",
    "        return entries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2f3ba9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_xx_recursive_to_df(directory, max_pages=1000):\n",
    "    \"\"\"\n",
    "    指定されたディレクトリを再帰的に検索し、HTMLファイルからテキストを抽出し、\n",
    "    ディレクトリ構造をキーとしてDataFrameに格納する関数\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    page_count = 0\n",
    "    \n",
    "    for root, _dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if re.match(r\"^d-\\d+\\.html?$\", file):\n",
    "                file_path = os.path.join(root, file)\n",
    "                stories = extract_all_entries_from_html(file_path)\n",
    "                if len(stories) == 0: continue\n",
    "                # ディレクトリ構造と抽出したテキストをDataFrameに格納するためリストに追加\n",
    "                for index, story in enumerate(stories, 1):\n",
    "                    data.append({\n",
    "                        \"directory\": root,\n",
    "                        # \"dirs\": _dirs,\n",
    "                        \"file\": file,\n",
    "                        \"story_index\": index,\n",
    "                        \"title\": story[\"title\"],\n",
    "                        \"issue_info\": story[\"issue_info\"],\n",
    "                        \"body_text\": story[\"body_text\"]\n",
    "                    })\n",
    "                    \n",
    "                page_count += 1\n",
    "                if page_count >= max_pages:\n",
    "                    print(f\"Processed {max_pages} pages. Stopping...\")\n",
    "                    return pd.DataFrame(data)  # DataFrameに変換して返す\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f5b5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行\n",
    "df = extract_d_xx_recursive_to_df(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c3ca0e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                directory      file  story_index        title  \\\n",
      "0  www.geocities.co.jp/Playtown-Dice/6159  d-1.html            1  未来の国からはるばると   \n",
      "1  www.geocities.co.jp/Playtown-Dice/6159  d-1.html            2    ドラえもんの大予言   \n",
      "2  www.geocities.co.jp/Playtown-Dice/6159  d-1.html            3      変身ビスケット   \n",
      "3  www.geocities.co.jp/Playtown-Dice/6159  d-1.html            4      秘スパイ大作戦   \n",
      "4  www.geocities.co.jp/Playtown-Dice/6159  d-1.html            5         コベアベ   \n",
      "\n",
      "                 issue_info                                          body_text  \n",
      "0  １５頁　小四７０年１月号（ドラえもんあらわれる）  お正月。のどかに自分の部屋でモチを食べる少年、野比のび太。そこへ突然どこからともなく、「三十...  \n",
      "1              １４頁　小四７０年２月号  いつものように学校から帰ってきて出かけようとするのび太を、ドラえもんが２２世紀のマジックハン...  \n",
      "2      ９頁　小六７４年４月号（動物ビスケット）  ママからお客にお菓子を買ってくるように頼まれたのび太。ドラえもんに行かせようとするがドラえも...  \n",
      "3              １４頁　小四７０年５月号  誤って先生のカビンを割ってしまったのび太。のび太は先生に謝りに行こうとするが、それを見ていた...  \n",
      "4               ９頁　小四７２年４月号  自分の部屋を片付けないでしずかのところへ出かけようとするのび太を叱るママ。その時ドラえもんが...  \n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=[\"file\", \"story_index\"], inplace=True, ignore_index=True)\n",
    "# データフレームを表示\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1c3516d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"doraemon_stories.csv\", index=False, escapechar='\\\\', quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
